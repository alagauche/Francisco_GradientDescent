Name:               Francisco McGee
Assignment:         Gradient Descent
Github:             https://github.com/alagauche/Francisco_GradientDescent.git

INSTRUCTIONS:
Only relevant file here is the ipython notebook, please just
open it and follow along:

    Francisco_GradientDescent.py


CHALLENGES/QUESTIONS:
I did not really have the time to explore this topic too much in-depth, 
but I am grateful to have gotten enough time to learn even a shallow amount.

I didn't seem to see a change in how changing the learning rate affected
performance of stochastic. What I changed was the "step" parameter, which 
is what determines the size of the steps we take to reach a
(local) minimum. I didn't see any changes between 1 to 0.0001, so I'm 
wondering if that's what I would expect with stochastic gradient 
descent or not.



